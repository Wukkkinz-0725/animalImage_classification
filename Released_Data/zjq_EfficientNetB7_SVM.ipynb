{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Wukkkinz-0725/animalImage_classification.git"
      ],
      "metadata": {
        "id": "p0Y0iPo8SjIX",
        "outputId": "4e468d9c-c261-41c7-8d33-0c4dfcc5d75d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "p0Y0iPo8SjIX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'animalImage_classification'...\n",
            "remote: Enumerating objects: 18607, done.\u001b[K\n",
            "remote: Counting objects: 100% (18607/18607), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 18607 (delta 18561), reused 18581 (delta 18549), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (18607/18607), 13.80 MiB | 23.74 MiB/s, done.\n",
            "Resolving deltas: 100% (18561/18561), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q efficientnet_pytorch"
      ],
      "metadata": {
        "id": "Cdj9jVAxjm6v",
        "outputId": "53e461ff-f24b-4265-9ae6-e743cb0d7584",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Cdj9jVAxjm6v",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, BatchSampler, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "S_Q8363gRlIE"
      },
      "id": "S_Q8363gRlIE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('./animalImage_classification/Released_Data')"
      ],
      "metadata": {
        "id": "DCG4pGJHR4QO"
      },
      "id": "DCG4pGJHR4QO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline"
      ],
      "metadata": {
        "id": "ANiA5CLoC_E2"
      },
      "id": "ANiA5CLoC_E2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c370d643-46fd-4d03-bb17-a875e79d5e2c",
      "metadata": {
        "id": "c370d643-46fd-4d03-bb17-a875e79d5e2c"
      },
      "outputs": [],
      "source": [
        "# Create Dataset class for multilabel classification\n",
        "class MultiClassImageDataset(Dataset):\n",
        "    def __init__(self, ann_df, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.ann_df = ann_df\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ann_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.ann_df['image'][idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        super_idx = self.ann_df['superclass_index'][idx]\n",
        "        super_label = self.super_map_df['class'][super_idx]\n",
        "\n",
        "        sub_idx = self.ann_df['subclass_index'][idx]\n",
        "        sub_label = self.sub_map_df['class'][sub_idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, super_idx, super_label, sub_idx, sub_label\n",
        "\n",
        "class MultiClassImageTestDataset(Dataset):\n",
        "    def __init__(self, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): # Count files in img_dir\n",
        "        return len([fname for fname in os.listdir(self.img_dir)])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = str(idx) + '.jpg'\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7398553-8842-4ad8-b348-767921a22482",
      "metadata": {
        "id": "e7398553-8842-4ad8-b348-767921a22482"
      },
      "outputs": [],
      "source": [
        "train_ann_df = pd.read_csv('train_data.csv')\n",
        "super_map_df = pd.read_csv('superclass_mapping.csv')\n",
        "sub_map_df = pd.read_csv('subclass_mapping.csv')\n",
        "\n",
        "train_img_dir = 'train_shuffle'\n",
        "test_img_dir = 'test_shuffle'\n",
        "\n",
        "image_preprocessing = transforms.Compose([\n",
        "        transforms.RandomRotation(10),      # rotate +/- 10 degrees\n",
        "        transforms.RandomHorizontalFlip(),  # reverse 50% of images\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create train and val split\n",
        "train_dataset = MultiClassImageDataset(train_ann_df, super_map_df, sub_map_df, train_img_dir, transform=image_preprocessing)\n",
        "train_dataset, val_dataset = random_split(train_dataset, [0.9, 0.1])\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = MultiClassImageTestDataset(super_map_df, sub_map_df, test_img_dir, transform=image_preprocessing)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)\n",
        "\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                         batch_size=1,\n",
        "                         shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_transform_single_image(image_path, transform):\n",
        "    image = Image.open(image_path).convert('RGB')  # Ensure it's read in RGB format\n",
        "    image = transform(image)\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "PQEB38eRlGaY"
      },
      "id": "PQEB38eRlGaY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf33a131-0c66-40dc-b8d4-ba5d0f840840",
      "metadata": {
        "id": "bf33a131-0c66-40dc-b8d4-ba5d0f840840"
      },
      "outputs": [],
      "source": [
        "# # Simple CNN\n",
        "# class CNN(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.block1 = nn.Sequential(\n",
        "#                         nn.Conv2d(3, 32, 3, padding='same'),\n",
        "#                         nn.ReLU(),\n",
        "#                         nn.BatchNorm2d(32),\n",
        "#                         nn.Conv2d(32, 32, 3, padding='same'),\n",
        "#                         nn.ReLU(),\n",
        "#                         nn.BatchNorm2d(32),\n",
        "#                         nn.Conv2d(32, 32, 3, padding='same'),\n",
        "#                         nn.ReLU(),\n",
        "#                         nn.BatchNorm2d(32),\n",
        "#                         nn.MaxPool2d(2, 2)\n",
        "#                       )\n",
        "\n",
        "#         self.block2 = nn.Sequential(\n",
        "#                         nn.Conv2d(32, 64, 3, padding='same'),\n",
        "#                         nn.ReLU(),\n",
        "#                         nn.BatchNorm2d(64),\n",
        "#                         nn.Conv2d(64, 64, 3, padding='same'),\n",
        "#                         nn.ReLU(),\n",
        "#                         nn.BatchNorm2d(64),\n",
        "#                         nn.Conv2d(64, 64, 3, padding='same'),\n",
        "#                         nn.ReLU(),\n",
        "#                         nn.BatchNorm2d(64),\n",
        "#                         nn.MaxPool2d(2, 2)\n",
        "#                       )\n",
        "\n",
        "#         self.block3 = nn.Sequential(\n",
        "#                         nn.Conv2d(64, 128, 3, padding='same'),\n",
        "#                         nn.ReLU(),\n",
        "#                         nn.BatchNorm2d(128),\n",
        "#                         nn.Conv2d(128, 128, 3, padding='same'),\n",
        "#                         nn.ReLU(),\n",
        "#                         nn.BatchNorm2d(128),\n",
        "#                         nn.Conv2d(128, 128, 3, padding='same'),\n",
        "#                         nn.ReLU(),\n",
        "#                         nn.BatchNorm2d(128),\n",
        "#                         nn.MaxPool2d(2, 2)\n",
        "#                       )\n",
        "\n",
        "#         self.fc1 = nn.Linear(4*4*128, 256)\n",
        "#         self.fc2 = nn.Linear(256, 128)\n",
        "#         self.fc3a = nn.Linear(128, 4)\n",
        "#         self.fc3b = nn.Linear(128, 88)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.block1(x)\n",
        "#         x = self.block2(x)\n",
        "#         x = self.block3(x)\n",
        "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         super_out = self.fc3a(x)\n",
        "#         sub_out = self.fc3b(x)\n",
        "#         return super_out, sub_out\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cuda', train_super=True, train_sub=True):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.train_super = train_super\n",
        "        self.train_sub = train_sub\n",
        "\n",
        "    def train_epoch(self):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            inputs, super_labels, sub_labels = data[0].to(self.device), data[1].to(self.device), data[3].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            if self.train_super and self.train_sub:\n",
        "                super_outputs, sub_outputs = self.model(inputs)\n",
        "            elif self.train_super:\n",
        "                super_outputs = self.model(inputs)\n",
        "            else:\n",
        "                sub_outputs = self.model(inputs)\n",
        "            loss = 0\n",
        "            if self.train_super:\n",
        "                loss += self.criterion(super_outputs, super_labels)\n",
        "            if self.train_sub:\n",
        "                loss += self.criterion(sub_outputs, sub_labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Training loss: {running_loss / len(self.train_loader):.3f}')\n",
        "\n",
        "    def validate_epoch(self):\n",
        "        super_correct, sub_correct, total = 0, 0, 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.val_loader):\n",
        "                inputs, super_labels, sub_labels = data[0].to(self.device), data[1].to(self.device), data[3].to(self.device)\n",
        "\n",
        "                if self.train_super and self.train_sub:\n",
        "                    super_outputs, sub_outputs = self.model(inputs)\n",
        "                elif self.train_super:\n",
        "                    super_outputs = self.model(inputs)\n",
        "                else:\n",
        "                    sub_outputs = self.model(inputs)\n",
        "                loss = 0\n",
        "                if self.train_super:\n",
        "                    loss += self.criterion(super_outputs, super_labels)\n",
        "                    _, super_predicted = torch.max(super_outputs.data, 1)\n",
        "                    super_correct += (super_predicted == super_labels).sum().item()\n",
        "                if self.train_sub:\n",
        "                    loss += self.criterion(sub_outputs, sub_labels)\n",
        "                    _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
        "                    sub_correct += (sub_predicted == sub_labels).sum().item()\n",
        "                total += super_labels.size(0)\n",
        "                running_loss += loss.item()\n",
        "\n",
        "        print(f'Validation Loss: {running_loss / len(self.val_loader):.3f}')\n",
        "        if self.train_super:\n",
        "            print(f'Validation Superclass Accuracy: {100 * super_correct / total:.2f} %')\n",
        "        if self.train_sub:\n",
        "            print(f'Validation Subclass Accuracy: {100 * sub_correct / total:.2f} %')\n",
        "\n",
        "    def test(self, save_to_csv=False, return_predictions=False):\n",
        "        self.model.eval()\n",
        "        if not self.test_loader:\n",
        "            raise NotImplementedError('test_loader not specified')\n",
        "\n",
        "        superclass_predictions = {'image': [], 'superclass_index': []}\n",
        "        subclass_predictions = {'image': [], 'subclass_index': []}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.test_loader):\n",
        "                inputs, img_name = data[0].to(self.device), data[1]\n",
        "\n",
        "                if self.train_super and self.train_sub:\n",
        "                    super_outputs, sub_outputs = self.model(inputs)\n",
        "                elif self.train_super:\n",
        "                    super_outputs = self.model(inputs)\n",
        "                    sub_outputs = None\n",
        "                elif self.train_sub:\n",
        "                    super_outputs = None\n",
        "                    sub_outputs = self.model(inputs)\n",
        "                else:\n",
        "                    continue  # Skip if neither superclass nor subclass is trained\n",
        "\n",
        "                if self.train_super:\n",
        "                    super_probs = F.softmax(super_outputs, dim=1)\n",
        "                    superclass_predictions['superclass_probs'].append(super_probs.cpu().numpy())\n",
        "                    superclass_predictions['image'].append(img_name[0])\n",
        "\n",
        "                if self.train_sub:\n",
        "                    sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "                    subclass_predictions['subclass_probs'].append(sub_probs.cpu().numpy())\n",
        "                    subclass_predictions['image'].append(img_name[0])\n",
        "\n",
        "        if save_to_csv:\n",
        "            if self.train_super:\n",
        "                superclass_df = pd.DataFrame(data=superclass_predictions)\n",
        "                superclass_df.to_csv('superclass_prediction.csv', index=False)\n",
        "\n",
        "            if self.train_sub:\n",
        "                subclass_df = pd.DataFrame(data=subclass_predictions)\n",
        "                subclass_df.to_csv('subclass_prediction.csv', index=False)\n",
        "\n",
        "        if return_predictions:\n",
        "            return superclass_predictions, subclass_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class CNNAutoencoder(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#         # Encoder (Similar to your existing CNN)\n",
        "#         self.encoder = nn.Sequential(\n",
        "#             nn.Conv2d(3, 32, 3, padding='same'), nn.ReLU(), nn.BatchNorm2d(32),\n",
        "#             nn.Conv2d(32, 32, 3, padding='same'), nn.ReLU(), nn.BatchNorm2d(32),\n",
        "#             nn.Conv2d(32, 32, 3, padding='same'), nn.ReLU(), nn.BatchNorm2d(32),\n",
        "#             nn.MaxPool2d(2, 2),\n",
        "#             nn.Conv2d(32, 64, 3, padding='same'), nn.ReLU(), nn.BatchNorm2d(64),\n",
        "#             nn.Conv2d(64, 64, 3, padding='same'), nn.ReLU(), nn.BatchNorm2d(64),\n",
        "#             nn.Conv2d(64, 64, 3, padding='same'), nn.ReLU(), nn.BatchNorm2d(64),\n",
        "#             nn.MaxPool2d(2, 2),\n",
        "#             nn.Conv2d(64, 128, 3, padding='same'), nn.ReLU(), nn.BatchNorm2d(128),\n",
        "#             nn.Conv2d(128, 128, 3, padding='same'), nn.ReLU(), nn.BatchNorm2d(128),\n",
        "#             nn.Conv2d(128, 128, 3, padding='same'), nn.ReLU(), nn.BatchNorm2d(128),\n",
        "#             nn.MaxPool2d(2, 2)\n",
        "#         )\n",
        "\n",
        "#         # Decoder (Reverses the encoder)\n",
        "#         self.decoder = nn.Sequential(\n",
        "#             nn.ConvTranspose2d(128, 128, 2, stride=2), nn.ReLU(), nn.BatchNorm2d(128),\n",
        "#             nn.ConvTranspose2d(128, 64, 2, stride=2), nn.ReLU(), nn.BatchNorm2d(64),\n",
        "#             nn.ConvTranspose2d(64, 32, 2, stride=2), nn.ReLU(), nn.BatchNorm2d(32),\n",
        "#             nn.Conv2d(32, 3, 3, padding='same')  # Output channels = 3 (RGB)\n",
        "#         )\n",
        "\n",
        "#         # Classification layers\n",
        "#         self.fc1 = nn.Linear(4*4*128, 256)\n",
        "#         self.fc2 = nn.Linear(256, 128)\n",
        "#         self.fc3a = nn.Linear(128, 4)    # Superclass\n",
        "#         self.fc3b = nn.Linear(128, 88)   # Subclass\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         encoded = self.encoder(x)\n",
        "#         decoded = self.decoder(encoded)\n",
        "\n",
        "#         x = torch.flatten(encoded, 1)  # Flatten for classification\n",
        "\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         super_out = self.fc3a(x)\n",
        "#         sub_out = self.fc3b(x)\n",
        "#         return super_out, sub_out, decoded\n",
        "\n",
        "\n",
        "\n",
        "# class Trainer():\n",
        "#     def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cuda', train_super=True, train_sub=True):\n",
        "#         self.model = model\n",
        "#         self.criterion = criterion\n",
        "#         self.optimizer = optimizer\n",
        "#         self.train_loader = train_loader\n",
        "#         self.val_loader = val_loader\n",
        "#         self.test_loader = test_loader\n",
        "#         self.device = device\n",
        "#         self.train_super = train_super\n",
        "#         self.train_sub = train_sub\n",
        "\n",
        "#     def train_epoch_prev(self):\n",
        "#         running_loss = 0.0\n",
        "#         for i, data in enumerate(self.train_loader):\n",
        "#             inputs, super_labels, sub_labels = data[0].to(self.device), data[1].to(self.device), data[3].to(self.device)\n",
        "\n",
        "#             self.optimizer.zero_grad()\n",
        "#             if self.train_super and self.train_sub:\n",
        "#                 super_outputs, sub_outputs = self.model(inputs)\n",
        "#             elif self.train_super:\n",
        "#                 super_outputs = self.model(inputs)\n",
        "#             else:\n",
        "#                 sub_outputs = self.model(inputs)\n",
        "#             loss = 0\n",
        "#             if self.train_super:\n",
        "#                 loss += self.criterion(super_outputs, super_labels)\n",
        "#             if self.train_sub:\n",
        "#                 loss += self.criterion(sub_outputs, sub_labels)\n",
        "#             loss.backward()\n",
        "#             self.optimizer.step()\n",
        "\n",
        "#             running_loss += loss.item()\n",
        "\n",
        "#         print(f'Training loss: {running_loss / len(self.train_loader):.3f}')\n",
        "#     def train_epoch(self):\n",
        "#         running_loss = 0.0\n",
        "#         for i, data in enumerate(self.train_loader):\n",
        "#             inputs, super_labels, sub_labels = data[0].to(self.device), data[1].to(self.device), data[3].to(self.device)\n",
        "\n",
        "#             self.optimizer.zero_grad()\n",
        "\n",
        "#             super_outputs, sub_outputs, reconstructed = self.model(inputs)\n",
        "\n",
        "#             # Classification loss\n",
        "#             loss = 0\n",
        "#             if self.train_super:\n",
        "#                 loss += self.criterion(super_outputs, super_labels)\n",
        "#             if self.train_sub:\n",
        "#                 loss += self.criterion(sub_outputs, sub_labels)\n",
        "\n",
        "#             # Reconstruction loss (e.g., MSE for autoencoder)\n",
        "#             reconstruction_loss = F.mse_loss(reconstructed, inputs)\n",
        "#             loss += reconstruction_loss\n",
        "\n",
        "#             loss.backward()\n",
        "#             self.optimizer.step()\n",
        "\n",
        "#             running_loss += loss.item()\n",
        "\n",
        "#         print(f'Training loss: {running_loss / len(self.train_loader):.3\n",
        "\n",
        "#     def validate_epoch(self):\n",
        "#         super_correct, sub_correct, total = 0, 0, 0\n",
        "#         running_loss = 0.0\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             for i, data in enumerate(self.val_loader):\n",
        "#                 inputs, super_labels, sub_labels = data[0].to(self.device), data[1].to(self.device), data[3].to(self.device)\n",
        "\n",
        "#                 if self.train_super and self.train_sub:\n",
        "#                     super_outputs, sub_outputs = self.model(inputs)\n",
        "#                 elif self.train_super:\n",
        "#                     super_outputs = self.model(inputs)\n",
        "#                 else:\n",
        "#                     sub_outputs = self.model(inputs)\n",
        "#                 loss = 0\n",
        "#                 if self.train_super:\n",
        "#                     loss += self.criterion(super_outputs, super_labels)\n",
        "#                     _, super_predicted = torch.max(super_outputs.data, 1)\n",
        "#                     super_correct += (super_predicted == super_labels).sum().item()\n",
        "#                 if self.train_sub:\n",
        "#                     loss += self.criterion(sub_outputs, sub_labels)\n",
        "#                     _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
        "#                     sub_correct += (sub_predicted == sub_labels).sum().item()\n",
        "#                 total += super_labels.size(0)\n",
        "#                 running_loss += loss.item()\n",
        "\n",
        "#         print(f'Validation Loss: {running_loss / len(self.val_loader):.3f}')\n",
        "#         if self.train_super:\n",
        "#             print(f'Validation Superclass Accuracy: {100 * super_correct / total:.2f} %')\n",
        "#         if self.train_sub:\n",
        "#             print(f'Validation Subclass Accuracy: {100 * sub_correct / total:.2f} %')\n",
        "\n",
        "#     def test(self, save_to_csv=False, return_predictions=False):\n",
        "#         self.model.eval()\n",
        "#         if not self.test_loader:\n",
        "#             raise NotImplementedError('test_loader not specified')\n",
        "\n",
        "#         superclass_predictions = {'image': [], 'superclass_index': []}\n",
        "#         subclass_predictions = {'image': [], 'subclass_index': []}\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             for i, data in enumerate(self.test_loader):\n",
        "#                 inputs, img_name = data[0].to(self.device), data[1]\n",
        "\n",
        "#                 if self.train_super and self.train_sub:\n",
        "#                     super_outputs, sub_outputs = self.model(inputs)\n",
        "#                 elif self.train_super:\n",
        "#                     super_outputs = self.model(inputs)\n",
        "#                     sub_outputs = None\n",
        "#                 elif self.train_sub:\n",
        "#                     super_outputs = None\n",
        "#                     sub_outputs = self.model(inputs)\n",
        "#                 else:\n",
        "#                     continue  # Skip if neither superclass nor subclass is trained\n",
        "\n",
        "#                 if self.train_super:\n",
        "#                     super_probs = F.softmax(super_outputs, dim=1)\n",
        "#                     superclass_predictions['superclass_probs'].append(super_probs.cpu().numpy())\n",
        "#                     superclass_predictions['image'].append(img_name[0])\n",
        "\n",
        "#                 if self.train_sub:\n",
        "#                     sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "#                     subclass_predictions['subclass_probs'].append(sub_probs.cpu().numpy())\n",
        "#                     subclass_predictions['image'].append(img_name[0])\n",
        "\n",
        "#         if save_to_csv:\n",
        "#             if self.train_super:\n",
        "#                 superclass_df = pd.DataFrame(data=superclass_predictions)\n",
        "#                 superclass_df.to_csv('superclass_prediction.csv', index=False)\n",
        "\n",
        "#             if self.train_sub:\n",
        "#                 subclass_df = pd.DataFrame(data=subclass_predictions)\n",
        "#                 subclass_df.to_csv('subclass_prediction.csv', index=False)\n",
        "\n",
        "#         if return_predictions:\n",
        "#             return superclass_predictions, subclass_predictions\n"
      ],
      "metadata": {
        "id": "LyXPd72JqZr-"
      },
      "id": "LyXPd72JqZr-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# class CustomEfficientNetB7(nn.Module):\n",
        "#     def __init__(self, num_super_classes=4, num_sub_classes=88):\n",
        "#         super(CustomEfficientNetB7, self).__init__()\n",
        "#         # 加载预训练的EfficientNet-B7模型\n",
        "#         self.base_model = EfficientNet.from_pretrained('efficientnet-b7')\n",
        "\n",
        "#         # 获取原始最后一层的输入特征数\n",
        "#         in_features = self.base_model._fc.in_features\n",
        "\n",
        "#         # 为超类和子类定义分类头\n",
        "#         self.super_class_classifier = nn.Linear(in_features, num_super_classes)\n",
        "#         self.sub_class_classifier = nn.Linear(in_features, num_sub_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # 从基础模型提取特征\n",
        "#         features = self.base_model.extract_features(x)\n",
        "\n",
        "#         # 全局平均池化\n",
        "#         pooled_features = F.adaptive_avg_pool2d(features, 1).squeeze(-1).squeeze(-1)\n",
        "\n",
        "#         # 分类到超类和子类\n",
        "#         super_class_output = self.super_class_classifier(pooled_features)\n",
        "#         sub_class_output = self.sub_class_classifier(pooled_features)\n",
        "\n",
        "#         return super_class_output, sub_class_output\n",
        "\n",
        "# # 使用BCE损失函数\n",
        "# def custom_loss(super_class_output, super_labels, sub_class_output, sub_labels):\n",
        "#     bce_loss = nn.BCEWithLogitsLoss()\n",
        "#     super_class_loss = bce_loss(super_class_output, F.one_hot(super_labels, num_classes=4).float())\n",
        "#     sub_class_loss = bce_loss(sub_class_output, F.one_hot(sub_labels, num_classes=88).float())\n",
        "#     return super_class_loss + sub_class_loss\n"
      ],
      "metadata": {
        "id": "AYqENNBomuNV"
      },
      "id": "AYqENNBomuNV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "class ENB7AndOneClassSVM(nn.Module):\n",
        "    def __init__(self, num_super_classes=4, num_sub_classes=88):\n",
        "        super(ENB7AndOneClassSVM, self).__init__()\n",
        "        # 加载预训练的EfficientNet-B7模型\n",
        "        self.base_model = EfficientNet.from_pretrained('efficientnet-b7')\n",
        "\n",
        "        # 获取原始最后一层的输入特征数\n",
        "        in_features = self.base_model._fc.in_features\n",
        "\n",
        "        # 为超类和子类定义分类头\n",
        "        self.super_class_classifier = nn.Linear(in_features, num_super_classes)\n",
        "        self.sub_class_classifier = nn.Linear(in_features, num_sub_classes)\n",
        "\n",
        "    def forward(self, x, extract_features_only=False):\n",
        "        # 从基础模型提取特征\n",
        "        features = self.base_model.extract_features(x)\n",
        "\n",
        "        if extract_features_only:\n",
        "            features_list = []\n",
        "            pooled_features = F.adaptive_avg_pool2d(features, 1).squeeze(-1).squeeze(-1)\n",
        "            features_list.append(pooled_features.cpu().detach().numpy())\n",
        "            return features_list  # Return extracted features for One-Class SVM training\n",
        "\n",
        "\n",
        "        # 全局平均池化\n",
        "        pooled_features = F.adaptive_avg_pool2d(features, 1).squeeze(-1).squeeze(-1)\n",
        "\n",
        "        # 分类到超类和子类\n",
        "        super_class_output = self.super_class_classifier(pooled_features)\n",
        "        sub_class_output = self.sub_class_classifier(pooled_features)\n",
        "\n",
        "        return super_class_output, sub_class_output\n",
        "\n",
        "# 使用BCE损失函数\n",
        "def custom_loss(super_class_output, super_labels, sub_class_output, sub_labels):\n",
        "    bce_loss = nn.BCEWithLogitsLoss()\n",
        "    super_class_loss = bce_loss(super_class_output, F.one_hot(super_labels, num_classes=4).float())\n",
        "    sub_class_loss = bce_loss(sub_class_output, F.one_hot(sub_labels, num_classes=88).float())\n",
        "    return super_class_loss + sub_class_loss\n"
      ],
      "metadata": {
        "id": "Zf8QXgDOtXFp"
      },
      "id": "Zf8QXgDOtXFp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train One class SVM\n",
        "# Init model and trainer\n",
        "device = 'cuda'\n",
        "model = ENB7AndOneClassSVM().to(device)\n",
        "\n",
        "# Assuming train_loader is your DataLoader for training data\n",
        "for i, data in enumerate(train_loader):\n",
        "    inputs = data[0].to(device)\n",
        "\n",
        "# for inputs, _ in train_loader:\n",
        "#     inputs = inputs.to(device)\n",
        "    features_list = model.forward(inputs, extract_features_only=True)\n",
        "\n",
        "\n",
        "# Concatenate all features\n",
        "svm_features = np.concatenate(features_list, axis=0)\n",
        "\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "one_class_svm = OneClassSVM(kernel='rbf', gamma='auto').fit(svm_features)\n"
      ],
      "metadata": {
        "id": "4gIHe3OiujPu",
        "outputId": "3e360245-0b86-40e8-cc5d-fb23a7814f71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4gIHe3OiujPu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebdf524a-98bf-4d0b-9b63-2b2b7b87daa1",
      "metadata": {
        "id": "ebdf524a-98bf-4d0b-9b63-2b2b7b87daa1",
        "outputId": "8df71f3f-9863-4318-fc46-26225b6a1a36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b7\n"
          ]
        }
      ],
      "source": [
        "# Init model and trainer\n",
        "# redefine train loader\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)\n",
        "device = 'cuda'\n",
        "model = ENB7AndOneClassSVM().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7941c289-d9b1-4714-b788-898b3b889f58",
      "metadata": {
        "id": "7941c289-d9b1-4714-b788-898b3b889f58",
        "outputId": "aabe2e87-610d-4152-ba79-48726b8fd111",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Training loss: 0.768\n",
            "Validation Loss: 1.704\n",
            "Validation Superclass Accuracy: 93.99 %\n",
            "Validation Subclass Accuracy: 62.97 %\n",
            "\n",
            "Epoch 2\n",
            "Training loss: 0.743\n",
            "Validation Loss: 1.784\n",
            "Validation Superclass Accuracy: 94.78 %\n",
            "Validation Subclass Accuracy: 61.55 %\n",
            "\n",
            "Epoch 3\n",
            "Training loss: 0.632\n",
            "Validation Loss: 1.596\n",
            "Validation Superclass Accuracy: 93.67 %\n",
            "Validation Subclass Accuracy: 64.40 %\n",
            "\n",
            "Epoch 4\n",
            "Training loss: 0.618\n",
            "Validation Loss: 1.993\n",
            "Validation Superclass Accuracy: 94.15 %\n",
            "Validation Subclass Accuracy: 63.13 %\n",
            "\n",
            "Epoch 5\n",
            "Training loss: 0.525\n",
            "Validation Loss: 1.669\n",
            "Validation Superclass Accuracy: 96.52 %\n",
            "Validation Subclass Accuracy: 64.08 %\n",
            "\n",
            "Epoch 6\n",
            "Training loss: 0.522\n",
            "Validation Loss: 1.609\n",
            "Validation Superclass Accuracy: 95.57 %\n",
            "Validation Subclass Accuracy: 66.14 %\n",
            "\n",
            "Epoch 7\n",
            "Training loss: 0.405\n",
            "Validation Loss: 1.749\n",
            "Validation Superclass Accuracy: 94.15 %\n",
            "Validation Subclass Accuracy: 65.35 %\n",
            "\n",
            "Epoch 8\n",
            "Training loss: 0.427\n",
            "Validation Loss: 1.957\n",
            "Validation Superclass Accuracy: 94.62 %\n",
            "Validation Subclass Accuracy: 61.39 %\n",
            "\n",
            "Epoch 9\n",
            "Training loss: 0.425\n",
            "Validation Loss: 1.959\n",
            "Validation Superclass Accuracy: 95.89 %\n",
            "Validation Subclass Accuracy: 63.77 %\n",
            "\n",
            "Epoch 10\n",
            "Training loss: 0.424\n",
            "Validation Loss: 1.656\n",
            "Validation Superclass Accuracy: 95.25 %\n",
            "Validation Subclass Accuracy: 66.77 %\n",
            "\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    trainer.train_epoch()\n",
        "    trainer.validate_epoch()\n",
        "    print('')\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.eval()\n",
        "if not trainer.test_loader:\n",
        "    raise NotImplementedError('test_loader not specified')\n",
        "\n",
        "superclass_predictions = {'image': [], 'superclass_index': [], 'probs': []}\n",
        "subclass_predictions = {'image': [], 'subclass_index': [], 'probs': [], 'novel': []}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(trainer.test_loader):\n",
        "        inputs, img_name = data[0].to(trainer.device), data[1]\n",
        "        pooled_features = model.forward(inputs, extract_features_only=True)\n",
        "        # pooled_features = F.adaptive_avg_pool2d(features, 1).squeeze(-1).squeeze(-1)\n",
        "        # pooled_features = pooled_features.cpu().detach().numpy()\n",
        "\n",
        "        svm_predictions = one_class_svm.predict(pooled_features)\n",
        "        print(svm_predictions)\n",
        "        novel_class_pred = svm_predictions == -1\n",
        "\n",
        "        if trainer.train_super and trainer.train_sub:\n",
        "            super_outputs, sub_outputs = trainer.model(inputs)\n",
        "        elif trainer.train_super:\n",
        "            super_outputs = trainer.model(inputs)\n",
        "            sub_outputs = None\n",
        "        elif trainer.train_sub:\n",
        "            super_outputs = None\n",
        "            sub_outputs = trainer.model(inputs)\n",
        "        else:\n",
        "            continue  # Skip if neither superclass nor subclass is trained\n",
        "\n",
        "        if trainer.train_super:\n",
        "            super_probs = F.softmax(super_outputs, dim=1)\n",
        "            superclass_predictions['probs'].append(super_probs.cpu().numpy())\n",
        "            superclass_predictions['image'].append(img_name[0])\n",
        "\n",
        "        if trainer.train_sub:\n",
        "            sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "            subclass_predictions['probs'].append(sub_probs.cpu().numpy())\n",
        "            subclass_predictions['image'].append(img_name[0])\n",
        "            subclass_predictions['image'].append(novel_class_pred)\n",
        "\n",
        "if trainer.train_super:\n",
        "    superclass_df = pd.DataFrame(data=superclass_predictions)\n",
        "    superclass_df.to_csv('superclass_prediction_with_prob.csv', index=False)\n",
        "\n",
        "if trainer.train_sub:\n",
        "    subclass_df = pd.DataFrame(data=subclass_predictions)\n",
        "    subclass_df.to_csv('subclass_prediction_with_prob.csv', index=False)"
      ],
      "metadata": {
        "id": "bqXKQ8ThL4cV",
        "outputId": "3143364e-5811-48eb-9ede-55c221c4d6d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "id": "bqXKQ8ThL4cV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-f7623fc1c359>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# pooled_features = pooled_features.cpu().detach().numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0msvm_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_class_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnovel_class_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_predictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_classes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1794\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m         \"\"\"\n\u001b[0;32m-> 1796\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1797\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \"\"\"\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m             X = self._validate_data(\n\u001b[0m\u001b[1;32m    614\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    913\u001b[0m             )\n\u001b[1;32m    914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    916\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. OneClassSVM expected <= 2."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del superclass_predictions(['superclass_index'])"
      ],
      "metadata": {
        "id": "cq7XA-w8g4uV",
        "outputId": "f6445e36-8968-40c4-d12c-1a6546dfc6c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "id": "cq7XA-w8g4uV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-95-d04f21f15913>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    del superclass_predictions(['superclass_index'])\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot delete function call\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subclass_predictions.keys()"
      ],
      "metadata": {
        "id": "R20p-BELdnOE",
        "outputId": "553e3d3a-0b81-4047-f556-4cc81dfdd3dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "R20p-BELdnOE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['image', 'subclass_index', 'probs'])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_df = pd.DataFrame({'ID': subclass_predictions['image'], 'probs': subclass_predictions['probs']})"
      ],
      "metadata": {
        "id": "X1ABK6LMkoPo"
      },
      "id": "X1ABK6LMkoPo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_df.to_csv('sub_class_probs.csv', index=False)"
      ],
      "metadata": {
        "id": "AcHVtj1lkzyV"
      },
      "id": "AcHVtj1lkzyV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BLRAY1PRk2PI"
      },
      "id": "BLRAY1PRk2PI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}