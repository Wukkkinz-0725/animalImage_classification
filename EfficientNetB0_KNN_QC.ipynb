{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wukkkinz-0725/animalImage_classification/blob/master/EfficientNetB0_KNN_QC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Wukkkinz-0725/animalImage_classification.git"
      ],
      "metadata": {
        "id": "p0Y0iPo8SjIX",
        "outputId": "a02f8cb9-3ca2-44f6-d3d6-1d5048894c9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "p0Y0iPo8SjIX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'animalImage_classification'...\n",
            "remote: Enumerating objects: 18600, done.\u001b[K\n",
            "remote: Counting objects: 100% (18600/18600), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 18600 (delta 18556), reused 18582 (delta 18549), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (18600/18600), 13.78 MiB | 20.73 MiB/s, done.\n",
            "Resolving deltas: 100% (18556/18556), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, BatchSampler, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "S_Q8363gRlIE"
      },
      "id": "S_Q8363gRlIE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('./animalImage_classification/Released_Data')"
      ],
      "metadata": {
        "id": "DCG4pGJHR4QO"
      },
      "id": "DCG4pGJHR4QO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "cifar10_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Filter images evenly across remaining 7 classes\n",
        "class_limit = 3000 // 7  # Approximate number of images per class\n",
        "class_counts = {label: 0 for label in range(10) if label not in [2, 5, 6]}\n",
        "filtered_images = []\n",
        "\n",
        "for image, label in cifar10_dataset:\n",
        "    if label in class_counts and class_counts[label] < class_limit:\n",
        "        filtered_images.append((image, label))\n",
        "        class_counts[label] += 1\n",
        "    if all(count == class_limit for count in class_counts.values()):\n",
        "        break\n",
        "\n",
        "# Read existing data from CSV file or create a new DataFrame if the file doesn't exist\n",
        "csv_file = 'train_data.csv'\n",
        "if os.path.isfile(csv_file):\n",
        "    existing_df = pd.read_csv(csv_file)\n",
        "else:\n",
        "    existing_df = pd.DataFrame(columns=['image', 'superclass_index', 'subclass_index'])\n",
        "\n",
        "# Prepare CSV data for new images\n",
        "new_csv_data = {'image': [], 'superclass_index': [], 'subclass_index': []}\n",
        "if not os.path.exists('train_shuffle'):\n",
        "    os.makedirs('train_shuffle')\n",
        "\n",
        "for i, (image, _) in enumerate(filtered_images, start=6322):\n",
        "    file_name = f'{i}.jpg'\n",
        "    image_path = os.path.join('train_shuffle', file_name)\n",
        "    image = transforms.ToPILImage()(image)\n",
        "    image.save(image_path)\n",
        "\n",
        "    # Update CSV data\n",
        "    new_csv_data['image'].append(file_name)\n",
        "    new_csv_data['superclass_index'].append(3)\n",
        "    new_csv_data['subclass_index'].append(87)\n",
        "\n",
        "# Create a DataFrame from the new data and append it to the existing DataFrame\n",
        "new_df = pd.DataFrame(new_csv_data)\n",
        "combined_df = existing_df.append(new_df, ignore_index=True)\n",
        "\n",
        "# Write the combined DataFrame to the CSV file\n",
        "combined_df.to_csv(csv_file, index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vmnw-5NkMK5l",
        "outputId": "a0e15486-bbcd-4e2b-91bf-ee32e0d502ab"
      },
      "id": "Vmnw-5NkMK5l",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:15<00:00, 11185596.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-4f17ec5ab8b2>:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  combined_df = existing_df.append(new_df, ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_files(directory):\n",
        "    return len([name for name in os.listdir(directory) if os.path.isfile(os.path.join(directory, name))])\n",
        "\n",
        "# 指定文件夹路径\n",
        "folder_path = 'train_shuffle'\n",
        "\n",
        "# 数文件数量\n",
        "file_count = count_files(folder_path)\n",
        "print(f\"文件数量: {file_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRTNuRptM25Q",
        "outputId": "68df0f5d-7e5c-4eaa-a868-cfb360a42ac3"
      },
      "id": "pRTNuRptM25Q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "文件数量: 8322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c370d643-46fd-4d03-bb17-a875e79d5e2c",
      "metadata": {
        "id": "c370d643-46fd-4d03-bb17-a875e79d5e2c"
      },
      "outputs": [],
      "source": [
        "# Create Dataset class for multilabel classification\n",
        "class MultiClassImageDataset(Dataset):\n",
        "    def __init__(self, ann_df, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.ann_df = ann_df\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ann_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.ann_df['image'][idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        super_idx = self.ann_df['superclass_index'][idx]\n",
        "        super_label = self.super_map_df['class'][super_idx]\n",
        "\n",
        "        sub_idx = self.ann_df['subclass_index'][idx]\n",
        "        sub_label = self.sub_map_df['class'][sub_idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, super_idx, super_label, sub_idx, sub_label\n",
        "\n",
        "class MultiClassImageTestDataset(Dataset):\n",
        "    def __init__(self, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): # Count files in img_dir\n",
        "        return len([fname for fname in os.listdir(self.img_dir)])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = str(idx) + '.jpg'\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7398553-8842-4ad8-b348-767921a22482",
      "metadata": {
        "id": "e7398553-8842-4ad8-b348-767921a22482"
      },
      "outputs": [],
      "source": [
        "train_ann_df = pd.read_csv('train_data.csv')\n",
        "super_map_df = pd.read_csv('superclass_mapping.csv')\n",
        "sub_map_df = pd.read_csv('subclass_mapping.csv')\n",
        "\n",
        "train_img_dir = 'train_shuffle'\n",
        "test_img_dir = 'test_shuffle'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_preprocessing = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0), std=(1)),\n",
        "])"
      ],
      "metadata": {
        "id": "5I-gHk7Wl-Ie"
      },
      "id": "5I-gHk7Wl-Ie",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Augmentation and normalization for training\n",
        "# Just normalization for validation and testing\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation(15),  # Rotates the image by +/- 15 degrees\n",
        "    transforms.RandomHorizontalFlip(),  # Flips the image horizontally with 50% probability\n",
        "    transforms.RandomVerticalFlip(),  # Flips the image vertically with 50% probability\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly changes image brightness, contrast, saturation, and hue\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization values can be adjusted\n",
        "])\n",
        "\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "hZwsfMCTl_nm"
      },
      "id": "hZwsfMCTl_nm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and val split\n",
        "train_dataset = MultiClassImageDataset(train_ann_df, super_map_df, sub_map_df, train_img_dir, transform=train_transforms)\n",
        "train_dataset, val_dataset = random_split(train_dataset, [0.9, 0.1])\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = MultiClassImageTestDataset(super_map_df, sub_map_df, test_img_dir, transform=val_test_transforms)"
      ],
      "metadata": {
        "id": "pjW1Bq-TmLts"
      },
      "id": "pjW1Bq-TmLts",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)\n",
        "\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                         batch_size=1,\n",
        "                         shuffle=False)"
      ],
      "metadata": {
        "id": "GAuwkw6dmM5F"
      },
      "id": "GAuwkw6dmM5F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf33a131-0c66-40dc-b8d4-ba5d0f840840",
      "metadata": {
        "id": "bf33a131-0c66-40dc-b8d4-ba5d0f840840"
      },
      "outputs": [],
      "source": [
        "# Simple CNN\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "                        nn.Conv2d(3, 32, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(32),\n",
        "                        nn.Conv2d(32, 32, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(32),\n",
        "                        nn.Conv2d(32, 32, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(32),\n",
        "                        nn.MaxPool2d(2, 2)\n",
        "                      )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "                        nn.Conv2d(32, 64, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.Conv2d(64, 64, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.Conv2d(64, 64, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.MaxPool2d(2, 2)\n",
        "                      )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "                        nn.Conv2d(64, 128, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.Conv2d(128, 128, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.Conv2d(128, 128, 3, padding='same'),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.MaxPool2d(2, 2)\n",
        "                      )\n",
        "\n",
        "        self.fc1 = nn.Linear(4*4*128, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3a = nn.Linear(128, 4)\n",
        "        self.fc3b = nn.Linear(128, 88)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        super_out = self.fc3a(x)\n",
        "        sub_out = self.fc3b(x)\n",
        "        return super_out, sub_out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class CustomResNet18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomResNet18, self).__init__()\n",
        "        # Load the pre-trained ResNet18 model\n",
        "        self.base_model = models.resnet18(pretrained=True)\n",
        "\n",
        "        # ResNet18's last fc layer's in_features\n",
        "        in_features = self.base_model.fc.in_features\n",
        "\n",
        "        # Replace the original fully connected layer with new classifiers\n",
        "        self.super_class_classifier = nn.Linear(in_features, 4)   # For super class\n",
        "        self.sub_class_classifier = nn.Linear(in_features, 88)    # For sub class\n",
        "\n",
        "        # Replace the original fc layer with a dummy layer that just passes through the features\n",
        "        self.base_model.fc = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features from the base model\n",
        "        features = self.base_model(x)\n",
        "\n",
        "        # Classify into super and sub classes\n",
        "        super_class_output = self.super_class_classifier(features)\n",
        "        sub_class_output = self.sub_class_classifier(features)\n",
        "\n",
        "        return super_class_output, sub_class_output\n"
      ],
      "metadata": {
        "id": "HSnQcpU_E9al"
      },
      "id": "HSnQcpU_E9al",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cuda',superclass_threshold=0.9,subclass_threshold=0.3):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.superclass_threshold = superclass_threshold\n",
        "        self.subclass_threshold = subclass_threshold\n",
        "\n",
        "    def train_epoch(self):\n",
        "        running_loss = 0.0\n",
        "        device = self.device\n",
        "        self.model.train()\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            inputs, super_labels, sub_labels = data[0].to(device), data[1].to(device), data[3].to(device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            super_outputs, sub_outputs = self.model(inputs)\n",
        "            loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Training loss: {running_loss/i:.3f}')\n",
        "\n",
        "    def validate_epoch(self):\n",
        "        super_correct = 0\n",
        "        sub_correct = 0\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "        device = self.device\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.val_loader):\n",
        "                inputs, super_labels, sub_labels = data[0].to(device), data[1].to(device), data[3].to(device)\n",
        "\n",
        "                super_outputs, sub_outputs = self.model(inputs)\n",
        "                loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "                super_probs = torch.nn.functional.softmax(super_outputs, dim=1)\n",
        "                sub_probs = torch.nn.functional.softmax(sub_outputs, dim=1)\n",
        "                # print(super_probs)\n",
        "                # print(sub_probs)\n",
        "                _, super_predicted = torch.max(super_probs, 1)\n",
        "                _, sub_predicted = torch.max(sub_probs, 1)\n",
        "                # Apply threshold to determine if a class is novel\n",
        "                super_predicted[torch.max(super_probs, 1).values < self.superclass_threshold] = 3  # Novel superclass\n",
        "                sub_predicted[torch.max(sub_probs, 1).values < self.subclass_threshold] = 87       # Novel subclass\n",
        "\n",
        "                total += super_labels.size(0)\n",
        "                super_correct += (super_predicted == super_labels).sum().item()\n",
        "                sub_correct += (sub_predicted == sub_labels).sum().item()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "        print(f'Validation loss: {running_loss/i:.3f}')\n",
        "        print(f'Validation superclass acc: {100 * super_correct / total:.2f} %')\n",
        "        print(f'Validation subclass acc: {100 * sub_correct / total:.2f} %')\n",
        "\n",
        "    def test(self, save_to_csv=False, return_predictions=False):\n",
        "        if not self.test_loader:\n",
        "            raise NotImplementedError('test_loader not specified')\n",
        "\n",
        "        # Evaluate on test set, in this simple demo no special care is taken for novel/unseen classes\n",
        "        test_predictions = {'image': [], 'superclass_index': [], 'subclass_index': []}\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.test_loader):\n",
        "                inputs, img_name = data[0].to(device), data[1]\n",
        "\n",
        "                super_outputs, sub_outputs = self.model(inputs)\n",
        "                super_probs = torch.nn.functional.softmax(super_outputs, dim=1)\n",
        "                sub_probs = torch.nn.functional.softmax(sub_outputs, dim=1)\n",
        "                _, super_predicted = torch.max(super_probs, 1)\n",
        "                _, sub_predicted = torch.max(sub_probs, 1)\n",
        "\n",
        "\n",
        "                # Apply threshold to determine if a class is novel\n",
        "                super_predicted[torch.max(super_probs, 1).values < self.superclass_threshold] = 3  # Novel superclass\n",
        "                sub_predicted[torch.max(sub_probs, 1).values < self.subclass_threshold] = 87       # Novel subclass\n",
        "\n",
        "                test_predictions['image'].append(img_name[0])\n",
        "                test_predictions['superclass_index'].append(super_predicted.item())\n",
        "                test_predictions['subclass_index'].append(sub_predicted.item())\n",
        "\n",
        "        test_predictions = pd.DataFrame(data=test_predictions)\n",
        "        # Create separate DataFrames for superclass and subclass predictions\n",
        "        superclass_predictions_df = test_predictions[['image', 'superclass_index']].rename(columns={'image': 'ID', 'superclass_index': 'Target'})\n",
        "        subclass_predictions_df = test_predictions[['image', 'subclass_index']].rename(columns={'image': 'ID', 'subclass_index': 'Target'})\n",
        "        if save_to_csv:\n",
        "            superclass_predictions_df.to_csv('superclass_predict.csv', index=False)\n",
        "            subclass_predictions_df.to_csv('subclass_predict.csv', index=False)\n",
        "\n",
        "        if return_predictions:\n",
        "            return superclass_predictions_df, subclass_predictions_df"
      ],
      "metadata": {
        "id": "YOwed_mjRXsg"
      },
      "id": "YOwed_mjRXsg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cuda'):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "\n",
        "    def train_epoch(self):\n",
        "        running_loss = 0.0\n",
        "        device = self.device\n",
        "        self.model.train()\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            inputs, super_labels, sub_labels = data[0].to(device), data[1].to(device), data[3].to(device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            super_outputs, sub_outputs = self.model(inputs)\n",
        "            loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Training loss: {running_loss/i:.3f}')\n",
        "\n",
        "    def validate_epoch(self):\n",
        "        super_correct = 0\n",
        "        sub_correct = 0\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "        device = self.device\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.val_loader):\n",
        "                inputs, super_labels, sub_labels = data[0].to(device), data[1].to(device), data[3].to(device)\n",
        "\n",
        "                super_outputs, sub_outputs = self.model(inputs)\n",
        "                loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "                _, super_predicted = torch.max(super_outputs.data, 1)\n",
        "                _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
        "\n",
        "                total += super_labels.size(0)\n",
        "                super_correct += (super_predicted == super_labels).sum().item()\n",
        "                sub_correct += (sub_predicted == sub_labels).sum().item()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "        print(f'Validation loss: {running_loss/i:.3f}')\n",
        "        print(f'Validation superclass acc: {100 * super_correct / total:.2f} %')\n",
        "        print(f'Validation subclass acc: {100 * sub_correct / total:.2f} %')\n",
        "\n",
        "    def test(self, save_to_csv=False, return_predictions=False):\n",
        "        if not self.test_loader:\n",
        "            raise NotImplementedError('test_loader not specified')\n",
        "\n",
        "        # Evaluate on test set, in this simple demo no special care is taken for novel/unseen classes\n",
        "        test_predictions = {'image': [], 'superclass_index': [], 'subclass_index': []}\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.test_loader):\n",
        "                inputs, img_name = data[0].to(device), data[1]\n",
        "\n",
        "                super_outputs, sub_outputs = self.model(inputs)\n",
        "                _, super_predicted = torch.max(super_outputs.data, 1)\n",
        "                _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
        "\n",
        "                test_predictions['image'].append(img_name[0])\n",
        "                test_predictions['superclass_index'].append(super_predicted.item())\n",
        "                test_predictions['subclass_index'].append(sub_predicted.item())\n",
        "\n",
        "        test_predictions = pd.DataFrame(data=test_predictions)\n",
        "        # Create separate DataFrames for superclass and subclass predictions\n",
        "        superclass_predictions_df = test_predictions[['image', 'superclass_index']].rename(columns={'image': 'ID', 'superclass_index': 'Target'})\n",
        "        subclass_predictions_df = test_predictions[['image', 'subclass_index']].rename(columns={'image': 'ID', 'subclass_index': 'Target'})\n",
        "        if save_to_csv:\n",
        "            superclass_predictions_df.to_csv('superclass_predict.csv', index=False)\n",
        "            subclass_predictions_df.to_csv('subclass_predict.csv', index=False)\n",
        "\n",
        "        if return_predictions:\n",
        "            return superclass_predictions_df, subclass_predictions_df"
      ],
      "metadata": {
        "id": "QCsESq13Rfqn"
      },
      "id": "QCsESq13Rfqn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class CustomResNet34(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomResNet34, self).__init__()\n",
        "        # Load the pre-trained ResNet34 model\n",
        "        self.base_model = models.resnet34(pretrained=True)\n",
        "\n",
        "        # ResNet34's last fc layer's in_features\n",
        "        in_features = self.base_model.fc.in_features\n",
        "\n",
        "        # Replace the original fully connected layer with new classifiers\n",
        "        self.super_class_classifier = nn.Linear(in_features, 4)   # For super class\n",
        "        self.sub_class_classifier = nn.Linear(in_features, 88)    # For sub class\n",
        "\n",
        "        # Replace the original fc layer with a dummy layer that just passes through the features\n",
        "        self.base_model.fc = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features from the base model\n",
        "        features = self.base_model(x)\n",
        "\n",
        "        # Classify into super and sub classes\n",
        "        super_class_output = self.super_class_classifier(features)\n",
        "        sub_class_output = self.sub_class_classifier(features)\n",
        "\n",
        "        return super_class_output, sub_class_output\n"
      ],
      "metadata": {
        "id": "A4XSzYnErbha"
      },
      "id": "A4XSzYnErbha",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class CustomResNet50(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomResNet50, self).__init__()\n",
        "        # Load the pre-trained ResNet50 model\n",
        "        self.base_model = models.resnet50(pretrained=True)\n",
        "\n",
        "        # ResNet50's last fc layer's in_features\n",
        "        in_features = self.base_model.fc.in_features\n",
        "\n",
        "        # Replace the original fully connected layer with new classifiers\n",
        "        self.super_class_classifier = nn.Linear(in_features, 4)   # For super class\n",
        "        self.sub_class_classifier = nn.Linear(in_features, 88)    # For sub class\n",
        "\n",
        "        # Replace the original fc layer with a dummy layer that just passes through the features\n",
        "        self.base_model.fc = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features from the base model\n",
        "        features = self.base_model(x)\n",
        "\n",
        "        # Classify into super and sub classes\n",
        "        super_class_output = self.super_class_classifier(features)\n",
        "        sub_class_output = self.sub_class_classifier(features)\n",
        "\n",
        "        return super_class_output, sub_class_output\n"
      ],
      "metadata": {
        "id": "qBeI6lZ26aW8"
      },
      "id": "qBeI6lZ26aW8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class CustomResNet152(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomResNet152, self).__init__()\n",
        "        # Load the pre-trained ResNet152 model\n",
        "        self.base_model = models.resnet152(pretrained=True)\n",
        "        # Disable training for all layers\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        # ResNet152's last fc layer's in_features\n",
        "        in_features = self.base_model.fc.in_features\n",
        "\n",
        "        # Replace the original fully connected layer with new classifiers\n",
        "        self.super_class_classifier = nn.Linear(in_features, 4)   # For super class\n",
        "        self.sub_class_classifier = nn.Linear(in_features, 88)    # For sub class\n",
        "\n",
        "        # Replace the original fc layer with a dummy layer that just passes through the features\n",
        "        self.base_model.fc = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features from the base model\n",
        "        features = self.base_model(x)\n",
        "\n",
        "        # Classify into super and sub classes\n",
        "        super_class_output = self.super_class_classifier(features)\n",
        "        sub_class_output = self.sub_class_classifier(features)\n",
        "\n",
        "        return super_class_output, sub_class_output\n"
      ],
      "metadata": {
        "id": "9951-NQa2tdj"
      },
      "id": "9951-NQa2tdj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebdf524a-98bf-4d0b-9b63-2b2b7b87daa1",
      "metadata": {
        "id": "ebdf524a-98bf-4d0b-9b63-2b2b7b87daa1"
      },
      "outputs": [],
      "source": [
        "# Init model and trainer\n",
        "device = 'cuda'\n",
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "model = CustomResNet18().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQDvy01iKAgS",
        "outputId": "ac7e334d-cdfa-4581-d1d7-dbce2cb8cafc"
      },
      "id": "UQDvy01iKAgS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 72.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Try resNet34\n",
        "device = 'cuda'\n",
        "model = CustomResNet34().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZuZo0CFm9HH",
        "outputId": "3362f39a-8247-4d20-a9d3-af55f58e6e73"
      },
      "id": "mZuZo0CFm9HH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 94.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Try resNet50\n",
        "model = CustomResNet50().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g9QjTYa6eBk",
        "outputId": "8ae158db-a1b3-4344-cd84-a59e4f252fe9"
      },
      "id": "6g9QjTYa6eBk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 98.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Try resNet152\n",
        "device = 'cuda'\n",
        "model = CustomResNet152().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader)"
      ],
      "metadata": {
        "id": "fJSz50z_2263",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2d3ec8b-029a-4774-dc23-bc913e0c682f"
      },
      "id": "fJSz50z_2263",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
            "100%|██████████| 230M/230M [00:01<00:00, 128MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainer.model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pX2nxUOKRkW",
        "outputId": "c5dd00f5-d586-4e3a-fa42-a146ed28d4c4"
      },
      "id": "0pX2nxUOKRkW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomResNet18(\n",
            "  (base_model): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Identity()\n",
            "  )\n",
            "  (super_class_classifier): Linear(in_features=512, out_features=4, bias=True)\n",
            "  (sub_class_classifier): Linear(in_features=512, out_features=88, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7941c289-d9b1-4714-b788-898b3b889f58",
      "metadata": {
        "id": "7941c289-d9b1-4714-b788-898b3b889f58",
        "outputId": "a0cac09c-538f-4553-8acb-199e066e6726",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Training loss: 3.492\n",
            "Validation loss: 3.073\n",
            "Validation superclass acc: 79.05 %\n",
            "Validation subclass acc: 44.68 %\n",
            "\n",
            "Epoch 2\n",
            "Training loss: 2.640\n",
            "Validation loss: 2.799\n",
            "Validation superclass acc: 80.34 %\n",
            "Validation subclass acc: 45.76 %\n",
            "\n",
            "Epoch 3\n",
            "Training loss: 2.265\n",
            "Validation loss: 2.455\n",
            "Validation superclass acc: 84.64 %\n",
            "Validation subclass acc: 54.14 %\n",
            "\n",
            "Epoch 4\n",
            "Training loss: 2.105\n",
            "Validation loss: 2.986\n",
            "Validation superclass acc: 75.94 %\n",
            "Validation subclass acc: 43.39 %\n",
            "\n",
            "Epoch 5\n",
            "Training loss: 2.001\n",
            "Validation loss: 2.109\n",
            "Validation superclass acc: 86.79 %\n",
            "Validation subclass acc: 57.14 %\n",
            "\n",
            "Epoch 6\n",
            "Training loss: 1.835\n",
            "Validation loss: 2.129\n",
            "Validation superclass acc: 87.22 %\n",
            "Validation subclass acc: 58.54 %\n",
            "\n",
            "Epoch 7\n",
            "Training loss: 1.845\n",
            "Validation loss: 1.895\n",
            "Validation superclass acc: 89.37 %\n",
            "Validation subclass acc: 61.22 %\n",
            "\n",
            "Epoch 8\n",
            "Training loss: 1.772\n",
            "Validation loss: 1.867\n",
            "Validation superclass acc: 89.90 %\n",
            "Validation subclass acc: 61.76 %\n",
            "\n",
            "Epoch 9\n",
            "Training loss: 1.648\n",
            "Validation loss: 2.871\n",
            "Validation superclass acc: 80.24 %\n",
            "Validation subclass acc: 54.67 %\n",
            "\n",
            "Epoch 10\n",
            "Training loss: 1.786\n",
            "Validation loss: 1.989\n",
            "Validation superclass acc: 88.51 %\n",
            "Validation subclass acc: 59.18 %\n",
            "\n",
            "Epoch 11\n",
            "Training loss: 1.500\n",
            "Validation loss: 2.131\n",
            "Validation superclass acc: 86.36 %\n",
            "Validation subclass acc: 56.50 %\n",
            "\n",
            "Epoch 12\n",
            "Training loss: 1.405\n",
            "Validation loss: 1.786\n",
            "Validation superclass acc: 89.26 %\n",
            "Validation subclass acc: 63.59 %\n",
            "\n",
            "Epoch 13\n",
            "Training loss: 1.380\n",
            "Validation loss: 1.991\n",
            "Validation superclass acc: 87.65 %\n",
            "Validation subclass acc: 60.37 %\n",
            "\n",
            "Epoch 14\n",
            "Training loss: 1.572\n",
            "Validation loss: 1.696\n",
            "Validation superclass acc: 90.12 %\n",
            "Validation subclass acc: 65.41 %\n",
            "\n",
            "Epoch 15\n",
            "Training loss: 1.370\n",
            "Validation loss: 2.176\n",
            "Validation superclass acc: 85.50 %\n",
            "Validation subclass acc: 60.79 %\n",
            "\n",
            "Epoch 16\n",
            "Training loss: 1.557\n",
            "Validation loss: 2.019\n",
            "Validation superclass acc: 87.00 %\n",
            "Validation subclass acc: 59.40 %\n",
            "\n",
            "Epoch 17\n",
            "Training loss: 1.267\n",
            "Validation loss: 1.641\n",
            "Validation superclass acc: 90.55 %\n",
            "Validation subclass acc: 63.91 %\n",
            "\n",
            "Epoch 18\n",
            "Training loss: 1.147\n",
            "Validation loss: 1.620\n",
            "Validation superclass acc: 90.66 %\n",
            "Validation subclass acc: 64.45 %\n",
            "\n",
            "Epoch 19\n",
            "Training loss: 1.132\n",
            "Validation loss: 1.748\n",
            "Validation superclass acc: 89.58 %\n",
            "Validation subclass acc: 64.55 %\n",
            "\n",
            "Epoch 20\n",
            "Training loss: 1.166\n",
            "Validation loss: 2.260\n",
            "Validation superclass acc: 85.18 %\n",
            "Validation subclass acc: 60.04 %\n",
            "\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(20):\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    trainer.train_epoch()\n",
        "    trainer.validate_epoch()\n",
        "    print('')\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d17e37-1a08-4ae1-8517-a16ff4769622",
      "metadata": {
        "id": "16d17e37-1a08-4ae1-8517-a16ff4769622",
        "outputId": "6d0585d9-e456-42e0-8c17-34ad1d3bdd66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThis simple baseline scores the following test accuracy\\n\\nSuperclass Accuracy\\nOverall: 43.83 %\\nSeen: 61.11 %\\nUnseen: 0.00 %\\n\\nSubclass Accuracy\\nOverall: 2.03 %\\nSeen: 9.56 %\\nUnseen: 0.00 %\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "trainer.test(save_to_csv=True, return_predictions=True)\n",
        "\n",
        "'''\n",
        "This simple baseline scores the following test accuracy\n",
        "\n",
        "Superclass Accuracy\n",
        "Overall: 43.83 %\n",
        "Seen: 61.11 %\n",
        "Unseen: 0.00 %\n",
        "\n",
        "Subclass Accuracy\n",
        "Overall: 2.03 %\n",
        "Seen: 9.56 %\n",
        "Unseen: 0.00 %\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ab70fb9-6e14-49f1-b9bb-5f3da6807399",
      "metadata": {
        "id": "6ab70fb9-6e14-49f1-b9bb-5f3da6807399"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}